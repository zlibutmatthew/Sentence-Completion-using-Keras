{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Completion Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNBcvySGbZmDPsGG9vHOFcF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zlibutmatthew/Sentence-Completion-using-Keras/blob/main/Sentence_Completion_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQYtBuPRPxf9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, GRU, Embedding\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07IvQxIDR5cK"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h2ylKDnQgtM",
        "outputId": "b4042f9a-875f-4098-80ab-f2926bcdefdd"
      },
      "source": [
        "train_df1 = pd.read_csv('train.csv')\n",
        "train_ls1=train_df1['text'].tolist()\n",
        "train_ls1[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['it is todays experience that got hcc',\n",
              " 'meh  needed parts got attitude',\n",
              " 'went somewhere else got looked after',\n",
              " 'he was told it would be about $350',\n",
              " 'this is incredibly poor workmanship']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf-iYptwRKUW",
        "outputId": "07dc1153-1a09-46a0-d2ff-5fc4866f8925"
      },
      "source": [
        "test_df1 = pd.read_csv('test.csv')\n",
        "test_ls1=test_df1['Text'].tolist()\n",
        "print(len(test_ls1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj9-N43hR8NM"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zNm6Dl2RR0ek",
        "outputId": "e1591618-88b5-48ef-9906-6d614530f6fb"
      },
      "source": [
        "#unknown character at index 20\n",
        "test_ls1.pop(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'rented a 20’'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNz593x7SCZI",
        "outputId": "1dabf2db-fa8d-45e8-c423-df93df73504f"
      },
      "source": [
        "train_st=''\n",
        "for item in train_ls1:\n",
        "    train_st += ' ' + item + '.'\n",
        "\n",
        "print(len(train_st))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "415493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4iASTxTSHTD",
        "outputId": "a98a17ed-694c-436e-a6b3-752eb5860be1"
      },
      "source": [
        "# Join all the sentences together and extract the unique characters from the combined sentences\n",
        "chars = set(train_st)\n",
        "\n",
        "# Creating a dictionary that maps integers to the characters\n",
        "int2char = dict(enumerate(chars))\n",
        "\n",
        "# Creating another dictionary that maps characters to integers\n",
        "char2int = {char: ind for ind, char in int2char.items()}\n",
        "\n",
        "print(char2int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"'\": 0, ' ': 1, '€': 2, 'c': 3, '~': 4, '-': 5, ';': 6, 'l': 7, '?': 8, 'u': 9, 'é': 10, '(': 11, '8': 12, 'í': 13, 't': 14, '6': 15, 'z': 16, 'h': 17, '4': 18, 'g': 19, '5': 20, '@': 21, 'n': 22, ')': 23, '\"': 24, 's': 25, 'o': 26, 'd': 27, '!': 28, 'k': 29, 'q': 30, '%': 31, '+': 32, '7': 33, 'b': 34, 'j': 35, 'e': 36, '3': 37, '9': 38, ':': 39, '0': 40, 'à': 41, 'm': 42, 'y': 43, 'a': 44, '.': 45, 'v': 46, '1': 47, '$': 48, '&': 49, '=': 50, 'p': 51, 'x': 52, '*': 53, 'f': 54, 'i': 55, '#': 56, '2': 57, 'w': 58, '×': 59, 'r': 60}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gyiJo1ISKik",
        "outputId": "9c93f0c3-b212-4826-9220-6d0878db0bbf"
      },
      "source": [
        "def create_seq(text):\n",
        "    length = 30\n",
        "    sequences = list()\n",
        "    for i in range(length, len(text)):\n",
        "        if text[i-length-1] == ' ':\n",
        "            # select sequence of tokens\n",
        "            seq = text[i-length:i+1]\n",
        "            # store\n",
        "            sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# create sequences   \n",
        "sequences = create_seq(train_st)\n",
        "sequences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 80250\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['it is todays experience that go',\n",
              " 'is todays experience that got h',\n",
              " 'todays experience that got hcc.',\n",
              " 'experience that got hcc. meh  n',\n",
              " 'that got hcc. meh  needed parts',\n",
              " 'got hcc. meh  needed parts got ',\n",
              " 'hcc. meh  needed parts got atti',\n",
              " 'meh  needed parts got attitude.',\n",
              " ' needed parts got attitude. wen',\n",
              " 'needed parts got attitude. went']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VRa7gpwTDQM",
        "outputId": "aa0cb653-2737-4d68-9594-35ff1e9d6c11"
      },
      "source": [
        "# create a character mapping index\n",
        "# chars = sorted(list(set(data_new)))\n",
        "# mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [char2int[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences)\n",
        "sequences[0:1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[55,\n",
              "  14,\n",
              "  1,\n",
              "  55,\n",
              "  25,\n",
              "  1,\n",
              "  14,\n",
              "  26,\n",
              "  27,\n",
              "  44,\n",
              "  43,\n",
              "  25,\n",
              "  1,\n",
              "  36,\n",
              "  52,\n",
              "  51,\n",
              "  36,\n",
              "  60,\n",
              "  55,\n",
              "  36,\n",
              "  22,\n",
              "  3,\n",
              "  36,\n",
              "  1,\n",
              "  14,\n",
              "  17,\n",
              "  44,\n",
              "  14,\n",
              "  1,\n",
              "  19,\n",
              "  26]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUxz4IhBTIVj",
        "outputId": "12976f64-d30b-43c1-893a-02b2aba91bec"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(char2int)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (72225, 30) Val shape: (8025, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV0ayVzUTj1i"
      },
      "source": [
        "## Building and Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9L-ao5lTRHE",
        "outputId": "f8cacc49-5d0e-4d20-ec3e-6c1c82b044f3"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab, 50, input_length=30, trainable=True))\n",
        "model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dense(vocab, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "model.fit(X_tr, y_tr, epochs=30, verbose=2, validation_data=(X_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 30, 50)            3050      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 150)               90900     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 61)                9211      \n",
            "=================================================================\n",
            "Total params: 103,161\n",
            "Trainable params: 103,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "2258/2258 - 186s - loss: 2.2961 - acc: 0.3349 - val_loss: 2.0046 - val_acc: 0.4037\n",
            "Epoch 2/30\n",
            "2258/2258 - 192s - loss: 1.8832 - acc: 0.4412 - val_loss: 1.7890 - val_acc: 0.4715\n",
            "Epoch 3/30\n",
            "2258/2258 - 187s - loss: 1.7231 - acc: 0.4859 - val_loss: 1.6732 - val_acc: 0.5029\n",
            "Epoch 4/30\n",
            "2258/2258 - 187s - loss: 1.6283 - acc: 0.5085 - val_loss: 1.6235 - val_acc: 0.5154\n",
            "Epoch 5/30\n",
            "2258/2258 - 192s - loss: 1.5645 - acc: 0.5284 - val_loss: 1.5929 - val_acc: 0.5293\n",
            "Epoch 6/30\n",
            "2258/2258 - 186s - loss: 1.5214 - acc: 0.5399 - val_loss: 1.5634 - val_acc: 0.5363\n",
            "Epoch 7/30\n",
            "2258/2258 - 186s - loss: 1.4871 - acc: 0.5476 - val_loss: 1.5527 - val_acc: 0.5452\n",
            "Epoch 8/30\n",
            "2258/2258 - 186s - loss: 1.4595 - acc: 0.5546 - val_loss: 1.5328 - val_acc: 0.5517\n",
            "Epoch 9/30\n",
            "2258/2258 - 191s - loss: 1.4408 - acc: 0.5587 - val_loss: 1.5319 - val_acc: 0.5505\n",
            "Epoch 10/30\n",
            "2258/2258 - 187s - loss: 1.4220 - acc: 0.5641 - val_loss: 1.5303 - val_acc: 0.5526\n",
            "Epoch 11/30\n",
            "2258/2258 - 187s - loss: 1.4083 - acc: 0.5686 - val_loss: 1.5198 - val_acc: 0.5521\n",
            "Epoch 12/30\n",
            "2258/2258 - 194s - loss: 1.3943 - acc: 0.5729 - val_loss: 1.5036 - val_acc: 0.5610\n",
            "Epoch 13/30\n",
            "2258/2258 - 187s - loss: 1.3837 - acc: 0.5746 - val_loss: 1.5232 - val_acc: 0.5520\n",
            "Epoch 14/30\n",
            "2258/2258 - 187s - loss: 1.3732 - acc: 0.5763 - val_loss: 1.5159 - val_acc: 0.5570\n",
            "Epoch 15/30\n",
            "2258/2258 - 187s - loss: 1.3646 - acc: 0.5784 - val_loss: 1.5100 - val_acc: 0.5559\n",
            "Epoch 16/30\n",
            "2258/2258 - 187s - loss: 1.3572 - acc: 0.5812 - val_loss: 1.5147 - val_acc: 0.5565\n",
            "Epoch 17/30\n",
            "2258/2258 - 186s - loss: 1.3526 - acc: 0.5826 - val_loss: 1.5091 - val_acc: 0.5589\n",
            "Epoch 18/30\n",
            "2258/2258 - 186s - loss: 1.3462 - acc: 0.5840 - val_loss: 1.5117 - val_acc: 0.5578\n",
            "Epoch 19/30\n",
            "2258/2258 - 187s - loss: 1.3432 - acc: 0.5829 - val_loss: 1.5086 - val_acc: 0.5630\n",
            "Epoch 20/30\n",
            "2258/2258 - 187s - loss: 1.3334 - acc: 0.5872 - val_loss: 1.5203 - val_acc: 0.5586\n",
            "Epoch 21/30\n",
            "2258/2258 - 187s - loss: 1.3265 - acc: 0.5883 - val_loss: 1.5125 - val_acc: 0.5669\n",
            "Epoch 22/30\n",
            "2258/2258 - 186s - loss: 1.3247 - acc: 0.5890 - val_loss: 1.5184 - val_acc: 0.5583\n",
            "Epoch 23/30\n",
            "2258/2258 - 186s - loss: 1.3212 - acc: 0.5875 - val_loss: 1.5100 - val_acc: 0.5524\n",
            "Epoch 24/30\n",
            "2258/2258 - 186s - loss: 1.3159 - acc: 0.5896 - val_loss: 1.5032 - val_acc: 0.5665\n",
            "Epoch 25/30\n",
            "2258/2258 - 186s - loss: 1.3167 - acc: 0.5892 - val_loss: 1.5099 - val_acc: 0.5635\n",
            "Epoch 26/30\n",
            "2258/2258 - 186s - loss: 1.3125 - acc: 0.5924 - val_loss: 1.5127 - val_acc: 0.5640\n",
            "Epoch 27/30\n",
            "2258/2258 - 187s - loss: 1.3072 - acc: 0.5933 - val_loss: 1.5108 - val_acc: 0.5646\n",
            "Epoch 28/30\n",
            "2258/2258 - 192s - loss: 1.3111 - acc: 0.5919 - val_loss: 1.5064 - val_acc: 0.5696\n",
            "Epoch 29/30\n",
            "2258/2258 - 187s - loss: 1.2994 - acc: 0.5956 - val_loss: 1.5025 - val_acc: 0.5600\n",
            "Epoch 30/30\n",
            "2258/2258 - 187s - loss: 1.2999 - acc: 0.5956 - val_loss: 1.5130 - val_acc: 0.5606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f12367fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEw8uZxQTpfO"
      },
      "source": [
        "## Predicting using the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jJYgc9JHTd1W"
      },
      "source": [
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict character\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        # reverse map integer to character\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += char\n",
        "    return in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JBfCzvVITgRI",
        "outputId": "b800a5de-4a18-48b2-f527-3919c8e33d6f"
      },
      "source": [
        "for item in test_ls1:\n",
        "    print([item, generate_seq(model, char2int, 30, item, 50)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-37a475e89dd4>:11: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "['awesome people to', 'awesome people to anyway are the best. we will be back. they are  s']\n",
            "['return was', 'return was a great people. we will be back. they are  staff ']\n",
            "['these guys are', 'these guys are the best. we will be back. they are  staff and ha']\n",
            "['i have  rented from them', 'i have  rented from them. they are  staff and had a great job. no stars wa']\n",
            "['what great individuals and', 'what great individuals and well come back. they are  staff and had a great j']\n",
            "['i look forward to', 'i look forward to anyway and i was not  be back. they are  staff an']\n",
            "['thanks for the', 'thanks for the staff and well come back. they are  staff and had']\n",
            "['thanks for a great weeke', 'thanks for a great weekend. they are  staff and had a great job. no stars ']\n",
            "['the customer service is', 'the customer service is a most park. they are  staff and had a great job.']\n",
            "['i have already recommended', 'i have already recommended. they are  staff and had a great job. no stars wa']\n",
            "['i highly', 'i highly recommend. they are  staff and had a great job. n']\n",
            "['best prices', 'best prices and in the park is a great job. no stars was not ']\n",
            "['they do their best to make the', 'they do their best to make the best. we will be back. they are  staff and had a ']\n",
            "['thats hard to', 'thats hard to anyway and i was not  be back. they are  staff an']\n",
            "['will rent from', 'will rent from them. they are  staff and had a great job. no sta']\n",
            "['second time renter from', 'second time renter from them. they are  staff and had a great job. no sta']\n",
            "['these guys are', 'these guys are the best. we will be back. they are  staff and ha']\n",
            "['thanks for being', 'thanks for being the staff and well come back. they are  staff and']\n",
            "['wow! what a great service', 'wow! what a great service. we will be back. they are  staff and had a great']\n",
            "['it  so worth', 'it  so worth. i was not  be back. they are  staff and had a gr']\n",
            "['their online reservation', 'their online reservation. we will be back. they are  staff and had a great']\n",
            "['the trailer tracked', 'the trailer tracked and they are not  be back. they are  staff and ha']\n",
            "['there were no white-knuckle', 'there were no white-knuckle. they are  staff and had a great job. no stars wa']\n",
            "['a completely uneventful', 'a completely uneventful. we will be back. they are  staff and had a great']\n",
            "['cost was reasonable', 'cost was reasonable. they are  staff and had a great job. no stars wa']\n",
            "['i definitely recommend', 'i definitely recommend. they are  staff and had a great job. no stars wa']\n",
            "['wanted to rent a trailer and', 'wanted to rent a trailer and in the park. they are  staff and had a great job.']\n",
            "['laura was', 'laura was not  be back. they are  staff and had a great job']\n",
            "['would definitely', 'would definitely recommend the staff and well  said the service wa']\n",
            "['you are greeted with a', 'you are greeted with a lot of park. i was not  be back. they are  staff ']\n",
            "['all paperwork is in order and', 'all paperwork is in order and i was not  be back. they are  staff and had a gre']\n",
            "['all rentals are clean and', 'all rentals are clean and in the park. they are  staff and had a great job.']\n",
            "['love renting from', 'love renting from them. they are  staff and had a great job. no sta']\n",
            "['i have used them 4 times and never', 'i have used them 4 times and never again. they are  staff and had a great job. no st']\n",
            "['recently  i rented a trailer', 'recently  i rented a trailer here. i was not  be back. they are  staff and had']\n",
            "['they are very helpful  friendly and', 'they are very helpful  friendly and it was not  be back. they are  staff and had a gr']\n",
            "['i will definitely rent from', 'i will definitely rent from them. they are  staff and had a great job. no sta']\n",
            "['best rv rental', 'best rv rental. i was not  be back. they are  staff and had a gr']\n",
            "['with that i had never', 'with that i had never again. they are  staff and had a great job. no st']\n",
            "['no run around  no hassles  no', 'no run around  no hassles  no appointment. they are  staff and had a great job.']\n",
            "['pleasant', 'pleasant. they are  staff and had a great job. no stars wa']\n",
            "['they even helped', 'they even helped me and it was not  be back. they are  staff and h']\n",
            "['upfront no hassles  no', 'upfront no hassles  no appointment. they are  staff and had a great job.']\n",
            "['this seemed rea', 'this seemed really and it was not  be back. they are  staff and h']\n",
            "['while 2 people were still', 'while 2 people were still not  be back. they are  staff and had a great job']\n",
            "['to be', 'to be back. they are  staff and had a great job. no sta']\n",
            "['time will', 'time will be back. they are  staff and had a great job. no ']\n",
            "['very unfortunate', 'very unfortunate. i was not  be back. they are  staff and had a gr']\n",
            "['touch up on engine door', 'touch up on engine door. we will be back. they are  staff and had a great']\n",
            "['bottom line: i purchased a', 'bottom line: i purchased and they were not  be back. they are  staff and had']\n",
            "['this charge was added by the rv dealer', 'this charge was added by the rv dealer. we will be back. they are  staff and had a great']\n",
            "['it went down hill', 'it went down hill in the staff. we were no sent leaking and they we']\n",
            "['i am  lucky if i put on 2000 miles', 'i am  lucky if i put on 2000 miles. i was not  be back. they are  staff and had a gr']\n",
            "['then  the amount of', 'then  the amount of the staff and well come back. they are  staff and']\n",
            "['a month at a time or more each', 'a month at a time or more each and i was not  be back. they are  staff and had a']\n",
            "['i literally have aged over', 'i literally have aged over the best. we will be back. they are  staff and ha']\n",
            "['after the money transfers', 'after the money transfers and i was not  be back. they are  staff and had a']\n",
            "['i have had 5 motorhomes in my', 'i have had 5 motorhomes in my experience. we will be back. they are  staff and ']\n",
            "['it will be weeks before', 'it will be weeks before and i was not  be back. they are  staff and had a']\n",
            "['join the backed', 'join the backed. we will be back. they are  staff and had a great']\n",
            "['never purchase from', 'never purchase from them. they are  staff and had a great job. no sta']\n",
            "['other then that the experience', 'other then that the experience. we will be back. they are  staff and had a great']\n",
            "['after the repair  the car', 'after the repair  the car  the worked the best problems was a great people.']\n",
            "['i asked them to', 'i asked them to anyway. they are  staff and had a great job. no s']\n",
            "['i did not  trust the', 'i did not  trust the best problems was a great people. we will be back']\n",
            "['when i drove away  the car was', 'when i drove away  the car was a great job. no stars was not  be back. they are ']\n",
            "['absolutely the best car care', 'absolutely the best car care of the staff. we were no sent leaking and they we']\n",
            "['professional and very', 'professional and very professional. we will be back. they are  staff an']\n",
            "['i will absolutely', 'i will absolutely recommend. they are  staff and had a great job. n']\n",
            "['i was not  about to', 'i was not  about to anyway and i was not  be back. they are  staff an']\n",
            "['i thought as i walked up to', 'i thought as i walked up to anyway and i was not  be back. they are  staff an']\n",
            "['i went back to my car and', 'i went back to my car and they were not  be back. they are  staff and had a']\n",
            "['he said  there  no', 'he said  there  no appointment. they are  staff and had a great job.']\n",
            "['you should call them back and', 'you should call them back and it was not  be back. they are  staff and had a gr']\n",
            "['i am  sure he will  treat', 'i am  sure he will  treat customer service was a great people. we will be b']\n",
            "['they fix the water pump and change', 'they fix the water pump and change. i was not  be back. they are  staff and had a gr']\n",
            "['the price was consistent with ', 'the price was consistent with them. they are  staff and had a great job. no star']\n",
            "['the service is', 'the service is a most park. they are  staff and had a great job.']\n",
            "['i will definitely', 'i will definitely be back. they are  staff and had a great job. no ']\n",
            "['although she has', 'although she has a great job. no stars was not  be back. they are ']\n",
            "['would not recommend this', 'would not recommend this place. they are  staff and had a great job. no st']\n",
            "['i drove the vehicle in', 'i drove the vehicle in the staff and well come back. they are  staff and']\n",
            "['end of', 'end of the staff and well  said the service was a great ']\n",
            "['top shelf all', 'top shelf all the work. i was not  be back. they are  staff and']\n",
            "['thanks again for the ', 'thanks again for the staff and well  said the service was a great peopl']\n",
            "['i took my 12 year old volvo there for a', 'i took my 12 year old volvo there for an and well  said the park is a great job. no stars']\n",
            "['thank you dawn for', 'thank you dawn for the staff and well come back. they are  staff and']\n",
            "['exceptionally rude on phone', 'exceptionally rude on phone call. i was not  be back. they are  staff and had']\n",
            "['sure glad i did not  have a', 'sure glad i did not  have and knowledgeably were friendly and it was not  be ']\n",
            "['so glad i do not  have to deal with', 'so glad i do not  have to deal with them. they are  staff and had a great job. no sta']\n",
            "['no reason to be rude', 'no reason to be rude. i was not  be back. they are  staff and had a gr']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7j0tcdcT5Lo"
      },
      "source": [
        "# if you need to save the model to load somewhere else:\n",
        "## model.save('Insert path here')\n",
        "# if you need to load the model\n",
        "## model = keras.models.load_model('Insert path here')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}